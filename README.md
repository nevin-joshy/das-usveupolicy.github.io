# Top-Down vs. Bottom-Up AI Governance: EU vs. US

**Authors:** William Sweet, Nevin Joshy, David Smith, William Shin, Matthew Lausev, Neha Keshan
**Date:** December 2025

## Abstract

### Problem Summary
The United States currently lacks a comprehensive policy for Artificial Intelligence. This lack of policy, puts in jeopardy the fundamental civil rights, liberties and safety of Americans. The main purpose of this lack of legislation, is to maintain an innovation-driven market and push forward America as a global leader in AI development. However this push for AI production has come at the cost of a dangerous level of deregulation for systems that have the power to control almost every factor of people's lives without their control or consent. We need to create AI legislation that promotes public safety, human rights, innovation and economic value, as innovation and safety should not be two mutually exclusive things.

We look to the EU and its release of the EU AI Act to serve as a guide for the legislation that we will be presenting. But a problem arises on what parts of its legislation can be taken and applied to the US, given the fact that the EU AI Act itself isn't perfect. While some view the EU AI Act as being detrimental to progress and that level of regulation being unrealistic for the US to adopt, there are certainly aspects that could be adopted and made to fit with our focus on progress and innovation. In order to protect Americans from biased and discriminatory systems, some proactive regulation must be put in place, rather than continuing reliance on our reactive system, before it is too late.

### Proposal Summary
To address the gaps left by current regulations and guidelines in the US, this paper proposes the establishment of a Federal AI Office in order to centralize oversight, monitor systems, and track risk. Rather than blanket regulations, the office will enforce an adapted version of the EU's risk-based model. This way, specific, high-impact sectors like housing, employment, and law enforcement can be regulated at a higher level. Additionally, the paper proposes both pre-market and post-market assessments to vet systems with many opportunities for intervention. By integrating these safeguards, this paper argues that the US can mitigate harmful outcomes with accelerated AI development without sacrificing market leadership.

## EU Approach

Firstly, we will give a brief overview of the EU approach to AI regulation, explaining their AI Act and how it could apply to US legislation. As the world's first comprehensive set of regulations on Artificial Intelligence, the EU's AI Act is a global test case for balancing innovation with the rights of citizens. The EU advertises this act as a "human-centric" approach; however, after a deeper analysis, it is evident that while strengths exist in its robust classification framework, there are pitfalls, including loopholes and implementation burdens. These strengths and weaknesses are necessary to evaluate, especially in order to develop a policy recommendation for the United States. One of the major strengths of the EU AI Act is its rejection of the blanket regulation that treats all systems the same in terms of enforcement and consequences. For example, AI implementation for a video game should not have the same consequences as one for loan approval, which can dramatically affect someone's life. The EU AI Act separates systems into four distinct categories, each with its own requirements.

* **Unacceptable Risk:** Any systems that do not comply with fundamental rights at any level. This could include systems explicitly created for social scoring or manipulation [2].
* **High Risk:** The most critical for adaptation for US policy, because companies in high-risk sectors are leveraging discriminatory models to take advantage of users. This includes infrastructure, law enforcement, employment, etc [2].
* **Limited Risk:** This includes systems like chatbots and deepfakes, which would be required to follow mandatory transparency guidelines to ensure users are aware of what they are interacting with [2].
* **Minimal Risk:** AI systems with limited impact, like spam filters, will not face any enforced regulation, rather encouraged to follow voluntary guidelines [2].

The tiered structure introduced by the EU theoretically protects the basic rights of its citizens without preventing the progress of the technology sector in Europe. However, with the development of General-Purpose AI (GPAI), the EU faced the need to make changes to the model due to the added layer of complexity, including documentation and copyright laws [3].

The EU AI Act has drawn a lot of criticism from a variety of groups, mainly due to its failure to protect the rights it was designed to protect. Some of these organizations include Access Now and the European Center for Not-for-Profit Law, which argue that it prioritizes state security rather than the liberties of its people [2]. These doubts are driven by gaps like the National Security Exemption, which removes law enforcement and national security from consequence and most of the established regulations. This is a major pitfall of the legislation because some of the most harmful use cases of AI systems, especially in the US, are through predictive policing and mass surveillance, which is not regulated through the EU AI Act. Additionally, there exists a loophole in Article 6(3), in that providers are allowed to audit the systems themselves [2]. This means that a company could simply argue that their system does not pose "significant harm", and can therefore avoid the strict requirements. By bringing the classification decision to companies themselves, the very foundation of the Act is hindered.

One of the most common challenges faced by countries in the EU in the face of incoming requirements outlined by the Act is meeting implementation deadlines. The Act has "extraterritorial scope", meaning that it applies to any provider that does business in the EU, regardless of location around the globe. Member States have consistently been missing the deadlines to create their own enforcement, with Italy being the only Member to pass a full national AI law [4]. Furthermore, many firms in Europe have voiced that many overlapping regulations, such as the GDPR and Data Act, make it very difficult for small and medium-sized companies to keep up with all of the impending regulation while continuing to scale. In response, the "Digital Omnibus" was proposed, to delay high-risk requirements until 2027 [3]. While the EU AI Act is a global leader in regulation and will influence global policy, it is important to recognize the strengths and weaknesses in order to develop a functional and relevant policy for the United States to adopt.

## US Approach & Recommendation

The legislation that the US currently operates under, as described earlier, primarily exists to maintain the US’s market leadership and innovation advantage in the AI field. It acts as a ‘minimalist regulator’, and has significant gaps when it comes to the protection of fundamental rights. To maintain the technological advantage while prioritizing the safety of its citizens, the US must establish a Federal AI Office. This office would not just coordinate research, but it would also adapt the EU’s risk-based classification framework and enforcement of conformity assessments to address the unchecked biases that currently exist [1]. The argument for a federal regulatory agency is seeded by the pitfalls of the current approach to prevent algorithmic discrimination. Without any oversight, the systems that are used for high-risk sectors like housing and law enforcement are just continuing systemic inequalities.

In an edition of the Loyola University Chicago International Law Review, Joseph uses specific statistics in their work to emphasize the necessity for federal involvement, noting that between 2008 and 2015, lenders that used algorithmic decision-making rejected over 1.3 million creditworthy Black and Hispanic applicants [1]. Additionally, Black neighborhoods have been appraised for 23% less than comparable homes in white neighborhoods [1]. Evidence like this confirms that the risks that AI poses are not just theoretical, but are very real and actively causing harm in communities around the United States. Therefore, we suggest the creation of a Federal AI Office in order to enforce standards across industry to prevent discriminatory impact, in line with our focus on public safety, human rights, innovation and economic value. The US policy should be adapted in three major ways: the risk-based classification system, mandatory conformity assessments, and monitoring with a ‘human in the loop’.

To begin, the “Risk-Based Approach” from the EU AI Act acts as the foundation for the legislation. Not all AI should be regulated equally, and by categorizing systems by their risk potential, proper enforcement can be applied. By formally defining what counts as high-risk and separating those systems from lower risk, the Federal AI Office will be able to focus regulatory resources on where it matters without stunting innovation and growth in sectors with low impact. We already see inklings of legislation similar to this in California's SB 53, which aims at categorizing AI developers based on the amount of money they produce yearly, " 'Large frontier developer' means a frontier developer that together with its affiliates collectively had annual gross revenues in excess of five hundred million dollars ($500,000,000) in the preceding calendar year" [13]. Though organizing strictly by profit for companies is not as expansive as the risk-based system of the EU AI Act, it is an attempt at categorization of AI production within the US. This is important to consider as federal law can be inspired by state law, and understanding the current landscape for AI regulation within the states is vital for forming a Federal AI Office.

Next, the Conformity Assessment (Article 19 of the EU AI Act) will require that providers have their system prove it meets specific requirements related to data governance, transparency, and accuracy [1]. The US currently follows a reactive approach, suing companies after incidents happen. However, a Federal AI Office would be proactive, and require these conformity assessments for high-risk algorithms to prevent any bias or discrimination before they are used in production. The data sets and training process will have to be documented thoroughly to prevent any gaps in coverage [1]. Similar to SB53, we also see inklings of this focus on transparency within H. B. 149 from Texas. This bill from Texas, seeks to "provide transparency regarding risks in the development", which is a great focus, and across the board of AI legislation within the US, transparency is among one of the more popular topics [14].

Finally, by enforcing monitoring after a system has reached the market, the Federal AI Office will have more comprehensive control over the privacy of users [1]. The Federal AI Office will be empowered to require providers to make automatic logs of events in their system. Additionally, it could mandate human oversight, ensuring that a human operator can intervene or override a decision that affects civil rights or liberties. The Office could also provide a channel for feedback, where users can report errors and issues to a human authority [1]. The United States cannot rely on the current patchwork of legislation long-term, and needs comprehensive regulation to protect the civil rights of vulnerable populations. A Federal AI Office would adapt the EU’s risk classification, assessments, and post-market requirements to ensure that the US doesn’t just have the world’s most advanced AI, but the most trustworthy as well.

## Collaboration

We collaborated with Group 3 (Public Facial Recognition), Group 1 (AI Bias in Hiring), and Group 6 (Deepfakes and Misinformation). These collaborative discussions allowed us to test our EU vs. US governance framework against some real world case studies.

### Collaboration with Group 3: Facial Recognition and Privacy

Our discussion with Group 3, who focused on public facial recognition, provided a contrast between the technical capabilities of surveillance tools and the legal frameworks constraining them. Group 3 centered their work on how organizations like Palantir are perceived and what their tools can do, whereas our group concentrated on the governing regulations. We observed that under the EU AI Act, remote biometric identification is largely classified as "High Risk" or prohibited for law enforcement except in very specific circumstances [5]. By comparison, what we discovered in the United States was a mix of city- and county-level rules instead of a unified federal prohibition.

While there is no federal guidance in the US, regulation still exists at the local level, with cities like Boston, Portland, and San Francisco instituting total bans [6]. We also noted in our conversation with Group 3 that, without a nationwide privacy framework, agencies such as the Department of Justice (DOJ) and the Federal Trade Commission (FTC) can only step in when particular cases of misuse or discrimination arise [7]. This stresses that while the EU views facial data as "explicitly sensitive" requiring proactive protection, the US legal system currently treats it as a useful law enforcement tool, regulated only reactively when specific harms occur.

### Collaboration with Group 1: AI Bias in Hiring

Our discussion with Group 1, who examined AI bias in hiring, highlighted the tension between ethical concerns over discrimination and the fragmented regulatory structures addressing them. Group 1's main point was that artificial intelligence frequently deepens systemic prejudice, making better Diversity, Equity, and Inclusion (DEI) safeguards essential. Our group focused on how the existing legal frameworks attempt to control these risks. We observed that under the EU AI Act, AI tools used for recruitment are labeled "High Risk," mandating strict adherence to data governance, transparency, and human review [8]. In contrast, the U.S. lacks a unified federal standard for regulating hiring algorithms.

Rather than a central law, the U.S. relies on broad anti-discrimination statutes, specific agency advice from bodies like the EEOC, and local measures such as NYC’s Local Law 144 [9]. This collaboration shows how this decentralized approach leaves major gaps in accountability. While the EU takes a preventative stance on hiring AI to mitigate potential damage, the U.S. is largely responsive, often patching up issues only after discrimination has taken place. We concluded that the U.S. would benefit from a federal “High Risk” classification for employment AI systems, modeled on the consistent transparency and auditing requirements already seen in EU and NYC but applied nationwide in a centralized fashion.

### Collaboration with Group 6: Deepfakes and Misinformation

Our discussion with Group 6, who focused on deepfakes and the spread of misinformation, revealed a clear contrast between technological solutions and the legal realities of regulating synthetic media. Whereas Group 6 prioritized the societal impact of misinformation and technical fixes, our team focused on the practicalities of government imposed rules.

Under the EU AI Act, AI generated political content must be clearly labeled, with enforcement supported by the Digital Services Act (DSA) to ensure platform compliance [10]. By comparison, the U.S. lacks comprehensive federal standards for deepfake regulation and instead relies on a fragmented collection of state-level laws, particularly in states such as Texas and California [11, 12]. This disjointed regulation leads to loopholes in enforcement, particularly because digital falsehoods easily cross state borders. Additionally, we noted that the First Amendment protections present a unique hurdle for the U.S., severely restricting the government’s power to mandate the kind of labeling enforced by the EU. Ultimately, we concluded that state-level measures are inadequate for handling the widespread scope of AI driven misinformation without a cohesive federal strategy.

## Conclusion

The United States’ current hands-off approach to artificial intelligence governance prioritizes rapid innovation at the expense of civil rights, transparency, and public trust, leaving Americans vulnerable to unchecked systems that are increasingly shaping daily life. While Europe’s risk-based regulatory framework through the EU AI Act may not be fully transferable to the U.S. economic and political landscape, its core principles provide a valuable foundation for meaningful reform. Establishing a Federal AI Office would provide the necessary structure to centralize oversight, identify system risks, and enforce targeted regulations across high-impact sectors such as housing, employment, and law enforcement. With the creation of a Federal AI Office, the U.S. could shift from a reactive to a proactive governance model, preventing harm rather than merely responding to it. Ultimately, this balanced approach ensures that innovation remains a driving force of economic growth while safeguarding the fundamental rights and freedoms of individuals in an increasingly automated society.

## Contributions

### Nevin Joshy
My contributions to the project started with research on global AI policy, starting with Japan for the mid-point update. After focusing our scope on EU vs. US policy, my contribution was mainly utilizing our research on EU strengths and US gaps in order to develop a policy recommendation for the United States. During our presentation, I created and presented the slides related to the policy recommendation, and for this paper, I wrote the US Approach & Recommendation section.

### William Shin
I managed the group collaboration efforts, coordinating discussions with Group 1 (Hiring), Group 3 (Facial Recognition), and Group 6 (Deepfakes). My primary contribution involved applying these governance frameworks to real-world use cases, specifically contrasting the EU’s "High Risk" model against the US's regulatory fragmentation. I documented these findings and wrote the collaboration section of this paper along with Matthew.

### William Sweet
I initially worked on researching US AI legislation, reading the AI Action Plan put out by the Trump administration as well as looking into state legislation regarding AI, particularly California, Texas and New York. For the essay specifically, I worked with Nevin for writing the US Approach as well as the problem/summary and proposal summary, and finally worked on revising and editing the document.

### Matthew Lausev
Specifically for this paper, I wrote the conclusion as well as helping William Shin with writing the contributions. Furthermore, I helped out William Sweet as well as David find resources and think up possible topics to mention when discussing EU and US AI legislation. I also worked on editing/revising the essay.

### David Smith
I worked majorly on researching EU AI policy, reading the EU AI Act, as well as understanding the implications of said Act as well as how it's affecting the EU. Furthermore, I worked with Nevin and William Sweet to understand the intersection of EU and US AI legislation, and how EU legislation could be adapted into US legislation. Finally, I helped with writing the conclusion and formatting the latex document.

### AI Usage
For this project, we used ChatGPT in assisting our data collection, especially regarding state AI law. Looking through every states legislation can require a lot of time, especially as state's define what exactly AI is differently, and some local legislations web pages boast more advanced searches while others are much more primitive.

---

### References

1.  Jean Joseph. "Should the United States Adopt Federal Artificial Intelligence Regulation Similar to the European Union". *Loyola University Chicago International Law Review*, Vol 20, No 1, pp. 105--122 (2023). [Link](https://lawecommons.luc.edu/cgi/viewcontent.cgi?article=1261&context=lucilr), [Accessed: December 1, 2025]
2.  European Parliament. "High-level Summary of the AI Act". (2024). [Link](https://artificialintelligenceact.eu/high-level-summary/), [Accessed: November 30, 2025]
3.  European Commission. "Digital Omnibus Regulation Proposal". (2025). [Link](https://digital-strategy.ec.europa.eu/en/library/digital-omnibus-regulation-proposal), [Accessed: November 30, 2025]
4.  Liam Blackford. "State of the Act: EU AI Act Implementation in Key Member States". (2025). [Link](https://www.jdsupra.com/legalnews/state-of-the-act-eu-ai-act-3732095/), [Accessed: November 30, 2025]
5.  European Commission. "Artificial Intelligence Act: Regulation of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence". (2024). [Link](https://artificialintelligenceact.eu/high-level-summary/), [Accessed: November 30, 2025]
6.  Powell, Meerah. "Portland Passes Nation’s Toughest Restriction on Facial Recognition Technology". *OPB*. (Sept 2020). [Link](https://www.opb.org/article/2020/09/09/portland-passes-nations-toughest-restriction-on-facial-recognition-technology/), [Accessed: December 2, 2025]
7.  CFPB, DOJ, EEOC, and FTC. "Joint Statement on Enforcement Efforts Against Discrimination and Bias in Automated Systems". *Federal Trade Commission*. (April 2023). [Link](https://www.ftc.gov/system/files/ftc_gov/pdf/EEOC-CRT-FTC-CFPB-AI-Joint-Statement(final).pdf), [Accessed: November 29, 2025]
8.  European Parliament. "Annex III: High-Risk AI Systems and List of Use Cases". *EU AI Act*. (2024). [Link](https://artificialintelligenceact.eu/annex/3/), [Accessed: November 28, 2025]
9.  New York City Department of Consumer and Worker Protection. "Local Law 144: Automated Employment Decision Tools". (2021). [Link](https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page), [Accessed: November 29, 2025]
10. European Commission. "Transparency obligations for providers and users of certain AI systems". (2024). [Link](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai), [Accessed: November 30, 2025]
11. Texas Legislature. "S.B. No. 751: An Act relating to the creation of a criminal offense for fabricating a deceptive video with intent to influence the outcome of an election". *Texas Election Code*. (2019). [Link](https://capitol.texas.gov/tlodocs/86R/billtext/html/SB00751F.htm), [Accessed: November 29, 2025]
12. California Legislature. "A.B. 730: Elections: deceptive audio or visual media". *California Elections Code*. (2019). [Link](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB730), [Accessed: December 1, 2025]
13. Scott Weiner. "SB 53, Wiener. Artificial intelligence models: large developers." (2025). [Link](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53), [Accessed: December 2, 2025]
14. Giovanni Capriglione. "TEXAS RESPONSIBLE ARTIFICIAL INTELLIGENCE GOVERNANCE ACT". (2025). [Link](https://capitol.texas.gov/tlodocs/89R/analysis/html/HB00149S.htm), [Accessed: December 1, 2025]
